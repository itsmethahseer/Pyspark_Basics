{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d97a34c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f5a6325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "55dbd548",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de1a4e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.Builder().appName(\"Myapp\").master('local').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "32af85ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-CBFSOGLM:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Myapp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x20343dd3c40>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376e4bb5",
   "metadata": {},
   "source": [
    "Now i created the object for my spark and created master as local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "96db2df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv(\"files/Advertisement.csv\")\n",
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f13fb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---------+-----+\n",
      "|_c0|  _c1|      _c2|  _c3|\n",
      "+---+-----+---------+-----+\n",
      "| TV|radio|newspaper|sales|\n",
      "|  1|230.1|     37.8| 69.2|\n",
      "|  2| 44.5|     39.3| 45.1|\n",
      "|  3| 17.2|     45.9| 69.3|\n",
      "|  4|151.5|     41.3| 58.5|\n",
      "|  5|180.8|     10.8| 58.4|\n",
      "|  6|  8.7|     48.9|   75|\n",
      "|  7| 57.5|     32.8| 23.5|\n",
      "|  8|120.2|     19.6| 11.6|\n",
      "|  9|  8.6|      2.1|    1|\n",
      "| 10|199.8|      2.6| 21.2|\n",
      "| 11| 66.1|      5.8| 24.2|\n",
      "| 12|214.7|       24|    4|\n",
      "| 13| 23.8|     35.1| 65.9|\n",
      "| 14| 97.5|      7.6|  7.2|\n",
      "| 15|204.1|     32.9|   46|\n",
      "| 16|195.4|     47.7| 52.9|\n",
      "| 17| 67.8|     36.6|  114|\n",
      "| 18|281.4|     39.6| 55.8|\n",
      "| 19| 69.2|     20.5| 18.3|\n",
      "+---+-----+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c4ff8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark2 = spark.read.option('header','true').csv(\"files/Advertisement.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7800761c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[TV: string, radio: string, newspaper: string, sales: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b66a464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---------+-----+\n",
      "| TV|radio|newspaper|sales|\n",
      "+---+-----+---------+-----+\n",
      "|  1|230.1|     37.8| 69.2|\n",
      "|  2| 44.5|     39.3| 45.1|\n",
      "|  3| 17.2|     45.9| 69.3|\n",
      "|  4|151.5|     41.3| 58.5|\n",
      "|  5|180.8|     10.8| 58.4|\n",
      "|  6|  8.7|     48.9|   75|\n",
      "|  7| 57.5|     32.8| 23.5|\n",
      "|  8|120.2|     19.6| 11.6|\n",
      "|  9|  8.6|      2.1|    1|\n",
      "| 10|199.8|      2.6| 21.2|\n",
      "| 11| 66.1|      5.8| 24.2|\n",
      "| 12|214.7|       24|    4|\n",
      "| 13| 23.8|     35.1| 65.9|\n",
      "| 14| 97.5|      7.6|  7.2|\n",
      "| 15|204.1|     32.9|   46|\n",
      "| 16|195.4|     47.7| 52.9|\n",
      "| 17| 67.8|     36.6|  114|\n",
      "| 18|281.4|     39.6| 55.8|\n",
      "| 19| 69.2|     20.5| 18.3|\n",
      "| 20|147.3|     23.9| 19.1|\n",
      "+---+-----+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fd733d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a4e3619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>radio</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>230.1</td>\n",
       "      <td>37.8</td>\n",
       "      <td>69.2</td>\n",
       "      <td>22.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44.5</td>\n",
       "      <td>39.3</td>\n",
       "      <td>45.1</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.2</td>\n",
       "      <td>45.9</td>\n",
       "      <td>69.3</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>151.5</td>\n",
       "      <td>41.3</td>\n",
       "      <td>58.5</td>\n",
       "      <td>18.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>180.8</td>\n",
       "      <td>10.8</td>\n",
       "      <td>58.4</td>\n",
       "      <td>12.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TV  radio  newspaper  sales\n",
       "1  230.1   37.8       69.2   22.1\n",
       "2   44.5   39.3       45.1   10.4\n",
       "3   17.2   45.9       69.3    9.3\n",
       "4  151.5   41.3       58.5   18.5\n",
       "5  180.8   10.8       58.4   12.9"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('files/Advertisement.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d30a37fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef7f616",
   "metadata": {},
   "source": [
    "we can see the difference in the datatype of both pandas dataframe and pyspark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ea98b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---------+-----+\n",
      "| TV|radio|newspaper|sales|\n",
      "+---+-----+---------+-----+\n",
      "|  1|230.1|     37.8| 69.2|\n",
      "|  2| 44.5|     39.3| 45.1|\n",
      "|  3| 17.2|     45.9| 69.3|\n",
      "+---+-----+---------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark2.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4e9e4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- TV: string (nullable = true)\n",
      " |-- radio: string (nullable = true)\n",
      " |-- newspaper: string (nullable = true)\n",
      " |-- sales: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0bfebf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark2 = spark.read.option('header','true').csv(\"files/Advertisement.csv\",inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0fb5273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- TV: integer (nullable = true)\n",
      " |-- radio: double (nullable = true)\n",
      " |-- newspaper: double (nullable = true)\n",
      " |-- sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4918a39",
   "metadata": {},
   "source": [
    "Now i can see the correct datatype in my dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7040f3c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TV', 'radio', 'newspaper', 'sales']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e6d1ef",
   "metadata": {},
   "source": [
    "like this you can see the columns in your dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f6a77c",
   "metadata": {},
   "source": [
    "if you want to see a specific column you can use select statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "934b8db8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---------+-----+\n",
      "| TV|radio|newspaper|sales|\n",
      "+---+-----+---------+-----+\n",
      "|  1|230.1|     37.8| 69.2|\n",
      "|  2| 44.5|     39.3| 45.1|\n",
      "|  3| 17.2|     45.9| 69.3|\n",
      "|  4|151.5|     41.3| 58.5|\n",
      "|  5|180.8|     10.8| 58.4|\n",
      "|  6|  8.7|     48.9| 75.0|\n",
      "|  7| 57.5|     32.8| 23.5|\n",
      "|  8|120.2|     19.6| 11.6|\n",
      "|  9|  8.6|      2.1|  1.0|\n",
      "| 10|199.8|      2.6| 21.2|\n",
      "| 11| 66.1|      5.8| 24.2|\n",
      "| 12|214.7|     24.0|  4.0|\n",
      "| 13| 23.8|     35.1| 65.9|\n",
      "| 14| 97.5|      7.6|  7.2|\n",
      "| 15|204.1|     32.9| 46.0|\n",
      "| 16|195.4|     47.7| 52.9|\n",
      "| 17| 67.8|     36.6|114.0|\n",
      "| 18|281.4|     39.6| 55.8|\n",
      "| 19| 69.2|     20.5| 18.3|\n",
      "| 20|147.3|     23.9| 19.1|\n",
      "+---+-----+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5db94f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(TV=1, radio=230.1, newspaper=37.8, sales=69.2),\n",
       " Row(TV=2, radio=44.5, newspaper=39.3, sales=45.1),\n",
       " Row(TV=3, radio=17.2, newspaper=45.9, sales=69.3)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3591bb0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| TV|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "| 20|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark2.select(\"TV\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f151d071",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| TV|radio|\n",
      "+---+-----+\n",
      "|  1|230.1|\n",
      "|  2| 44.5|\n",
      "|  3| 17.2|\n",
      "|  4|151.5|\n",
      "|  5|180.8|\n",
      "|  6|  8.7|\n",
      "|  7| 57.5|\n",
      "|  8|120.2|\n",
      "|  9|  8.6|\n",
      "| 10|199.8|\n",
      "| 11| 66.1|\n",
      "| 12|214.7|\n",
      "| 13| 23.8|\n",
      "| 14| 97.5|\n",
      "| 15|204.1|\n",
      "| 16|195.4|\n",
      "| 17| 67.8|\n",
      "| 18|281.4|\n",
      "| 19| 69.2|\n",
      "| 20|147.3|\n",
      "+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark2.select(['TV','radio']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "523bbc78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'TV'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark2['TV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e479995f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('TV', 'int'),\n",
       " ('radio', 'double'),\n",
       " ('newspaper', 'double'),\n",
       " ('sales', 'double')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "151d60e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+------------------+\n",
      "|summary|                TV|            radio|         newspaper|             sales|\n",
      "+-------+------------------+-----------------+------------------+------------------+\n",
      "|  count|               200|              200|               200|               200|\n",
      "|   mean|             100.5|         147.0425|23.264000000000024|30.553999999999995|\n",
      "| stddev|57.879184513951124|85.85423631490805|14.846809176168728| 21.77862083852283|\n",
      "|    min|                 1|              0.7|               0.0|               0.3|\n",
      "|    max|               200|            296.4|              49.6|             114.0|\n",
      "+-------+------------------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark2.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eb77d5",
   "metadata": {},
   "source": [
    "if i want to add columns into dataframe i can use withcolumn method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "23193f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---------+-----+-----------+\n",
      "| TV|radio|newspaper|sales|super sales|\n",
      "+---+-----+---------+-----+-----------+\n",
      "|  1|230.1|     37.8| 69.2|      138.4|\n",
      "|  2| 44.5|     39.3| 45.1|       90.2|\n",
      "|  3| 17.2|     45.9| 69.3|      138.6|\n",
      "|  4|151.5|     41.3| 58.5|      117.0|\n",
      "|  5|180.8|     10.8| 58.4|      116.8|\n",
      "|  6|  8.7|     48.9| 75.0|      150.0|\n",
      "|  7| 57.5|     32.8| 23.5|       47.0|\n",
      "|  8|120.2|     19.6| 11.6|       23.2|\n",
      "|  9|  8.6|      2.1|  1.0|        2.0|\n",
      "| 10|199.8|      2.6| 21.2|       42.4|\n",
      "| 11| 66.1|      5.8| 24.2|       48.4|\n",
      "| 12|214.7|     24.0|  4.0|        8.0|\n",
      "| 13| 23.8|     35.1| 65.9|      131.8|\n",
      "| 14| 97.5|      7.6|  7.2|       14.4|\n",
      "| 15|204.1|     32.9| 46.0|       92.0|\n",
      "| 16|195.4|     47.7| 52.9|      105.8|\n",
      "| 17| 67.8|     36.6|114.0|      228.0|\n",
      "| 18|281.4|     39.6| 55.8|      111.6|\n",
      "| 19| 69.2|     20.5| 18.3|       36.6|\n",
      "| 20|147.3|     23.9| 19.1|       38.2|\n",
      "+---+-----+---------+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf = df_pyspark2.withColumn(\"super sales\",df_pyspark2['sales']*2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eed0b7",
   "metadata": {},
   "source": [
    "Now i just created a extra column by multiplying my sales feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6997a55e",
   "metadata": {},
   "source": [
    "if you want to remove a column you can use drop method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5537b7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---------+\n",
      "| TV|radio|newspaper|\n",
      "+---+-----+---------+\n",
      "|  1|230.1|     37.8|\n",
      "|  2| 44.5|     39.3|\n",
      "|  3| 17.2|     45.9|\n",
      "|  4|151.5|     41.3|\n",
      "|  5|180.8|     10.8|\n",
      "|  6|  8.7|     48.9|\n",
      "|  7| 57.5|     32.8|\n",
      "|  8|120.2|     19.6|\n",
      "|  9|  8.6|      2.1|\n",
      "| 10|199.8|      2.6|\n",
      "| 11| 66.1|      5.8|\n",
      "| 12|214.7|     24.0|\n",
      "| 13| 23.8|     35.1|\n",
      "| 14| 97.5|      7.6|\n",
      "| 15|204.1|     32.9|\n",
      "| 16|195.4|     47.7|\n",
      "| 17| 67.8|     36.6|\n",
      "| 18|281.4|     39.6|\n",
      "| 19| 69.2|     20.5|\n",
      "| 20|147.3|     23.9|\n",
      "+---+-----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark2.drop('sales').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b81b7a7",
   "metadata": {},
   "source": [
    "if i want to rename your column's name i can use the below method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9b669447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+-----+\n",
      "|My TV|radio|newspaper|sales|\n",
      "+-----+-----+---------+-----+\n",
      "|    1|230.1|     37.8| 69.2|\n",
      "|    2| 44.5|     39.3| 45.1|\n",
      "|    3| 17.2|     45.9| 69.3|\n",
      "|    4|151.5|     41.3| 58.5|\n",
      "|    5|180.8|     10.8| 58.4|\n",
      "|    6|  8.7|     48.9| 75.0|\n",
      "|    7| 57.5|     32.8| 23.5|\n",
      "|    8|120.2|     19.6| 11.6|\n",
      "|    9|  8.6|      2.1|  1.0|\n",
      "|   10|199.8|      2.6| 21.2|\n",
      "|   11| 66.1|      5.8| 24.2|\n",
      "|   12|214.7|     24.0|  4.0|\n",
      "|   13| 23.8|     35.1| 65.9|\n",
      "|   14| 97.5|      7.6|  7.2|\n",
      "|   15|204.1|     32.9| 46.0|\n",
      "|   16|195.4|     47.7| 52.9|\n",
      "|   17| 67.8|     36.6|114.0|\n",
      "|   18|281.4|     39.6| 55.8|\n",
      "|   19| 69.2|     20.5| 18.3|\n",
      "|   20|147.3|     23.9| 19.1|\n",
      "+-----+-----+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark2.withColumnRenamed('TV','My TV').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a64e309",
   "metadata": {},
   "source": [
    "### pyspark handling missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b9432e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = spark.read.option('header','true').csv(\"files/New_set.csv\",inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4911a15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+------+------------------+\n",
      "|     City| Age|Salary|Eligible for Bonus|\n",
      "+---------+----+------+------------------+\n",
      "|   Mumbai|  27| 51000|               Yes|\n",
      "|   Mumbai|  30| 52000|                No|\n",
      "|  Newyork|  27| 48000|               Yes|\n",
      "|  Newyork|  29| 66000|                No|\n",
      "|    Tokyo|  48|  null|               Yes|\n",
      "|Singapore|  33| 69000|                No|\n",
      "|  Newyork|null| 79000|               Yes|\n",
      "|   Mumbai|  38|  null|               Yes|\n",
      "|Singapore|  35| 38000|                No|\n",
      "|    Tokyo|null| 56000|                No|\n",
      "|Singapore|  35| 72000|               Yes|\n",
      "|   Mumbai|  31| 65000|               Yes|\n",
      "|Singapore|  37| 49000|                No|\n",
      "+---------+----+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b4ed79ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------+------------------+\n",
      "|     City|Age|Salary|Eligible for Bonus|\n",
      "+---------+---+------+------------------+\n",
      "|   Mumbai| 27| 51000|               Yes|\n",
      "|   Mumbai| 30| 52000|                No|\n",
      "|  Newyork| 27| 48000|               Yes|\n",
      "|  Newyork| 29| 66000|                No|\n",
      "|Singapore| 33| 69000|                No|\n",
      "|Singapore| 35| 38000|                No|\n",
      "|Singapore| 35| 72000|               Yes|\n",
      "|   Mumbai| 31| 65000|               Yes|\n",
      "|Singapore| 37| 49000|                No|\n",
      "+---------+---+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf.na.drop().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a010e0",
   "metadata": {},
   "source": [
    "if i did'nt mention anything inside drop method it will delete all the rows with null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0d86f488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+------+------------------+\n",
      "|     City| Age|Salary|Eligible for Bonus|\n",
      "+---------+----+------+------------------+\n",
      "|   Mumbai|  27| 51000|               Yes|\n",
      "|   Mumbai|  30| 52000|                No|\n",
      "|  Newyork|  27| 48000|               Yes|\n",
      "|  Newyork|  29| 66000|                No|\n",
      "|    Tokyo|  48|  null|               Yes|\n",
      "|Singapore|  33| 69000|                No|\n",
      "|  Newyork|null| 79000|               Yes|\n",
      "|   Mumbai|  38|  null|               Yes|\n",
      "|Singapore|  35| 38000|                No|\n",
      "|    Tokyo|null| 56000|                No|\n",
      "|Singapore|  35| 72000|               Yes|\n",
      "|   Mumbai|  31| 65000|               Yes|\n",
      "|Singapore|  37| 49000|                No|\n",
      "+---------+----+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c559fdf1",
   "metadata": {},
   "source": [
    "it will not delete anything since there is no rows with all null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c6f5cf05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+------+------------------+\n",
      "|     City| Age|Salary|Eligible for Bonus|\n",
      "+---------+----+------+------------------+\n",
      "|   Mumbai|  27| 51000|               Yes|\n",
      "|   Mumbai|  30| 52000|                No|\n",
      "|  Newyork|  27| 48000|               Yes|\n",
      "|  Newyork|  29| 66000|                No|\n",
      "|    Tokyo|  48|  null|               Yes|\n",
      "|Singapore|  33| 69000|                No|\n",
      "|  Newyork|null| 79000|               Yes|\n",
      "|   Mumbai|  38|  null|               Yes|\n",
      "|Singapore|  35| 38000|                No|\n",
      "|    Tokyo|null| 56000|                No|\n",
      "|Singapore|  35| 72000|               Yes|\n",
      "|   Mumbai|  31| 65000|               Yes|\n",
      "|Singapore|  37| 49000|                No|\n",
      "+---------+----+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf.na.drop(how='any',thresh=2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7cc261",
   "metadata": {},
   "source": [
    "using thresh=2 it will delete the rows with less than two non null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "997ad29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+-------------+------------------+\n",
      "|     City|          Age|       Salary|Eligible for Bonus|\n",
      "+---------+-------------+-------------+------------------+\n",
      "|   Mumbai|           27|        51000|               Yes|\n",
      "|   Mumbai|           30|        52000|                No|\n",
      "|  Newyork|           27|        48000|               Yes|\n",
      "|  Newyork|           29|        66000|                No|\n",
      "|    Tokyo|           48|Missing value|               Yes|\n",
      "|Singapore|           33|        69000|                No|\n",
      "|  Newyork|Missing value|        79000|               Yes|\n",
      "|   Mumbai|           38|Missing value|               Yes|\n",
      "|Singapore|           35|        38000|                No|\n",
      "|    Tokyo|Missing value|        56000|                No|\n",
      "|Singapore|           35|        72000|               Yes|\n",
      "|   Mumbai|           31|        65000|               Yes|\n",
      "|Singapore|           37|        49000|                No|\n",
      "+---------+-------------+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf.na.fill('Missing value').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92c7a7d",
   "metadata": {},
   "source": [
    "if you want to only replace only in specific feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9e97a77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+------+------------------+\n",
      "|     City|          Age|Salary|Eligible for Bonus|\n",
      "+---------+-------------+------+------------------+\n",
      "|   Mumbai|           27| 51000|               Yes|\n",
      "|   Mumbai|           30| 52000|                No|\n",
      "|  Newyork|           27| 48000|               Yes|\n",
      "|  Newyork|           29| 66000|                No|\n",
      "|    Tokyo|           48|  null|               Yes|\n",
      "|Singapore|           33| 69000|                No|\n",
      "|  Newyork|Missing value| 79000|               Yes|\n",
      "|   Mumbai|           38|  null|               Yes|\n",
      "|Singapore|           35| 38000|                No|\n",
      "|    Tokyo|Missing value| 56000|                No|\n",
      "|Singapore|           35| 72000|               Yes|\n",
      "|   Mumbai|           31| 65000|               Yes|\n",
      "|Singapore|           37| 49000|                No|\n",
      "+---------+-------------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf.na.fill('Missing value','Age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "75ff49df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+------+------------------+\n",
      "|     City| Age|Salary|Eligible for Bonus|\n",
      "+---------+----+------+------------------+\n",
      "|   Mumbai|  27| 51000|               Yes|\n",
      "|   Mumbai|  30| 52000|                No|\n",
      "|  Newyork|  27| 48000|               Yes|\n",
      "|  Newyork|  29| 66000|                No|\n",
      "|    Tokyo|  48|  null|               Yes|\n",
      "|Singapore|  33| 69000|                No|\n",
      "|  Newyork|null| 79000|               Yes|\n",
      "|   Mumbai|  38|  null|               Yes|\n",
      "|Singapore|  35| 38000|                No|\n",
      "|    Tokyo|null| 56000|                No|\n",
      "|Singapore|  35| 72000|               Yes|\n",
      "|   Mumbai|  31| 65000|               Yes|\n",
      "|Singapore|  37| 49000|                No|\n",
      "+---------+----+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d8fb1cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3217e3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer(\n",
    "    inputCols=['Age','Salary'],\n",
    "    outputCols=['{}_imputed'.format(c) for c in ['Age','Salary']]\n",
    "    \n",
    ").setStrategy('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "013f7925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+------+------------------+\n",
      "|     City| Age|Salary|Eligible for Bonus|\n",
      "+---------+----+------+------------------+\n",
      "|   Mumbai|  27| 51000|               Yes|\n",
      "|   Mumbai|  30| 52000|                No|\n",
      "|  Newyork|  27| 48000|               Yes|\n",
      "|  Newyork|  29| 66000|                No|\n",
      "|    Tokyo|  48|  null|               Yes|\n",
      "|Singapore|  33| 69000|                No|\n",
      "|  Newyork|null| 79000|               Yes|\n",
      "|   Mumbai|  38|  null|               Yes|\n",
      "|Singapore|  35| 38000|                No|\n",
      "|    Tokyo|null| 56000|                No|\n",
      "|Singapore|  35| 72000|               Yes|\n",
      "|   Mumbai|  31| 65000|               Yes|\n",
      "|Singapore|  37| 49000|                No|\n",
      "+---------+----+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fa2d9e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "new = imputer.fit(newdf).transform(newdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3d96aa45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+------+------------------+-----------+--------------+\n",
      "|     City| Age|Salary|Eligible for Bonus|Age_imputed|Salary_imputed|\n",
      "+---------+----+------+------------------+-----------+--------------+\n",
      "|   Mumbai|  27| 51000|               Yes|         27|         51000|\n",
      "|   Mumbai|  30| 52000|                No|         30|         52000|\n",
      "|  Newyork|  27| 48000|               Yes|         27|         48000|\n",
      "|  Newyork|  29| 66000|                No|         29|         66000|\n",
      "|    Tokyo|  48|  null|               Yes|         48|         58636|\n",
      "|Singapore|  33| 69000|                No|         33|         69000|\n",
      "|  Newyork|null| 79000|               Yes|         33|         79000|\n",
      "|   Mumbai|  38|  null|               Yes|         38|         58636|\n",
      "|Singapore|  35| 38000|                No|         35|         38000|\n",
      "|    Tokyo|null| 56000|                No|         33|         56000|\n",
      "|Singapore|  35| 72000|               Yes|         35|         72000|\n",
      "|   Mumbai|  31| 65000|               Yes|         31|         65000|\n",
      "|Singapore|  37| 49000|                No|         37|         49000|\n",
      "+---------+----+------+------------------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ec7dea",
   "metadata": {},
   "source": [
    "### Filter methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a848d5",
   "metadata": {},
   "source": [
    "if i want to see only the salary greater than 50000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "37a9ee36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+------+------------------+-----------+--------------+\n",
      "|     City| Age|Salary|Eligible for Bonus|Age_imputed|Salary_imputed|\n",
      "+---------+----+------+------------------+-----------+--------------+\n",
      "|   Mumbai|  27| 51000|               Yes|         27|         51000|\n",
      "|   Mumbai|  30| 52000|                No|         30|         52000|\n",
      "|  Newyork|  29| 66000|                No|         29|         66000|\n",
      "|    Tokyo|  48|  null|               Yes|         48|         58636|\n",
      "|Singapore|  33| 69000|                No|         33|         69000|\n",
      "|  Newyork|null| 79000|               Yes|         33|         79000|\n",
      "|   Mumbai|  38|  null|               Yes|         38|         58636|\n",
      "|    Tokyo|null| 56000|                No|         33|         56000|\n",
      "|Singapore|  35| 72000|               Yes|         35|         72000|\n",
      "|   Mumbai|  31| 65000|               Yes|         31|         65000|\n",
      "+---------+----+------+------------------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new.filter('Salary_imputed>=50000').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8600eff",
   "metadata": {},
   "source": [
    "if i want to see only specific features with salary grater than 50000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "eed340a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+\n",
      "|Age_imputed| Age|\n",
      "+-----------+----+\n",
      "|         27|  27|\n",
      "|         30|  30|\n",
      "|         29|  29|\n",
      "|         48|  48|\n",
      "|         33|  33|\n",
      "|         33|null|\n",
      "|         38|  38|\n",
      "|         33|null|\n",
      "|         35|  35|\n",
      "|         31|  31|\n",
      "+-----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new.filter('Salary_imputed>=50000').select(['Age_imputed','Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f303668c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+------------------+-----------+--------------+\n",
      "|  City| Age|Salary|Eligible for Bonus|Age_imputed|Salary_imputed|\n",
      "+------+----+------+------------------+-----------+--------------+\n",
      "|Mumbai|  27| 51000|               Yes|         27|         51000|\n",
      "|Mumbai|  30| 52000|                No|         30|         52000|\n",
      "| Tokyo|null| 56000|                No|         33|         56000|\n",
      "+------+----+------+------------------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new.filter((new['Salary_imputed']>=50000) & (new['Salary_imputed']<=58000)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c6784d",
   "metadata": {},
   "source": [
    "### Groupby and aggregate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d8268b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "new = spark.read.csv('files/salary22.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1f18e673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+------+----+\n",
      "|    Name|    Department|Salary| _c3|\n",
      "+--------+--------------+------+----+\n",
      "|Thahseer|data scientist| 30000|null|\n",
      "|   Arjun|          Mern| 15000|null|\n",
      "|Thahseer|           IOT| 20000|null|\n",
      "|   Arjun|          Mean| 13000|null|\n",
      "|   Jerin|        Golang| 18000|null|\n",
      "|   Jerin|        Golang| 19000|null|\n",
      "|  Rashid|          Mern| 12000|null|\n",
      "|  Rashid|          Mean| 10000|null|\n",
      "+--------+--------------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2e701317",
   "metadata": {},
   "outputs": [],
   "source": [
    "new.groupBy('Name').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e67c2e7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|    Name|sum(Salary)|\n",
      "+--------+-----------+\n",
      "|Thahseer|      50000|\n",
      "|  Rashid|      22000|\n",
      "|   Jerin|      37000|\n",
      "|   Arjun|      28000|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1547e130",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "03ba141f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new = new.groupBy('Department').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7f709974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+\n",
      "|    Department|sum(Salary)|\n",
      "+--------------+-----------+\n",
      "|data scientist|      30000|\n",
      "|           IOT|      20000|\n",
      "|          Mean|      23000|\n",
      "|          Mern|      27000|\n",
      "|        Golang|      37000|\n",
      "+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "54228746",
   "metadata": {},
   "outputs": [],
   "source": [
    "new = new.groupBy(\"Name\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "67bf453c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|    Name|avg(Salary)|\n",
      "+--------+-----------+\n",
      "|Thahseer|    25000.0|\n",
      "|  Rashid|    11000.0|\n",
      "|   Jerin|    18500.0|\n",
      "|   Arjun|    14000.0|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "23de3c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "new = new.groupBy(\"Department\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2555370e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|    Department|count|\n",
      "+--------------+-----+\n",
      "|data scientist|    1|\n",
      "|           IOT|    1|\n",
      "|          Mean|    2|\n",
      "|          Mern|    2|\n",
      "|        Golang|    2|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "25586a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "new2 = new.agg({'Salary':'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "362170f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(Salary)|\n",
      "+-----------+\n",
      "|     137000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a91f88dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'Department', 'Salary', '_c3']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a4303c",
   "metadata": {},
   "source": [
    "#### if i want to convert my two input features into one independent features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "06602cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv('files/New_set.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e4758fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+------+------------------+\n",
      "|     City| Age|Salary|Eligible for Bonus|\n",
      "+---------+----+------+------------------+\n",
      "|   Mumbai|  27| 51000|               Yes|\n",
      "|   Mumbai|  30| 52000|                No|\n",
      "|  Newyork|  27| 48000|               Yes|\n",
      "|  Newyork|  29| 66000|                No|\n",
      "|    Tokyo|  48|  null|               Yes|\n",
      "|Singapore|  33| 69000|                No|\n",
      "|  Newyork|null| 79000|               Yes|\n",
      "|   Mumbai|  38|  null|               Yes|\n",
      "|Singapore|  35| 38000|                No|\n",
      "|    Tokyo|null| 56000|                No|\n",
      "|Singapore|  35| 72000|               Yes|\n",
      "|   Mumbai|  31| 65000|               Yes|\n",
      "|Singapore|  37| 49000|                No|\n",
      "+---------+----+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb45021",
   "metadata": {},
   "source": [
    "[Age,Salary]----> new feature--->independent feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a6c90c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "featureassembler=VectorAssembler(inputCols=[\"Age\",\"Salary\"],outputCol=\"Independent Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "44a13991",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = featureassembler.transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ee526a4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[City: string, Age: int, Salary: int, Eligible for Bonus: string, Independent Features: vector]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "dd3f2c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalized_data=output.select(\"Independent Features\",\"Salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9e98966b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Independent Features: vector, Salary: int]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972f42e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c415561",
   "metadata": {},
   "source": [
    "### Now i want to see part 6 continuation of krish naik pyspark tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ed2c82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9e7f7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc94becf",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.Builder().appName(\"Myapp\").master('local').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b407ff5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-CBFSOGLM:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Myapp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x298ecc990c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e99faa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = spark.read.csv(\"files/newnote.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32ddc77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|experience|salary|\n",
      "+---------+---+----------+------+\n",
      "|    krish| 31|        10| 30000|\n",
      "|sudhanshu| 30|         8| 25000|\n",
      "|    sunny| 29|         4| 20000|\n",
      "|     paul| 24|         3| 20000|\n",
      "|   harsha| 21|         1| 15000|\n",
      "|  shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6b83a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'age', 'experience', 'salary']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75290d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4ac3e9",
   "metadata": {},
   "source": [
    "using this VectorAssemler i will group my independent features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9e7209b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize = VectorAssembler(inputCols=['age','experience'],outputCol='Input Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ca83b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = vectorize.transform(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91487677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+--------------+\n",
      "|     Name|age|experience|salary|Input Features|\n",
      "+---------+---+----------+------+--------------+\n",
      "|    krish| 31|        10| 30000|   [31.0,10.0]|\n",
      "|sudhanshu| 30|         8| 25000|    [30.0,8.0]|\n",
      "|    sunny| 29|         4| 20000|    [29.0,4.0]|\n",
      "|     paul| 24|         3| 20000|    [24.0,3.0]|\n",
      "|   harsha| 21|         1| 15000|    [21.0,1.0]|\n",
      "|  shubham| 23|         2| 18000|    [23.0,2.0]|\n",
      "+---------+---+----------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16f76219",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalized_data = output.select(\"salary\",\"Input Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aca13a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+\n",
      "|salary|Input Features|\n",
      "+------+--------------+\n",
      "| 30000|   [31.0,10.0]|\n",
      "| 25000|    [30.0,8.0]|\n",
      "| 20000|    [29.0,4.0]|\n",
      "| 20000|    [24.0,3.0]|\n",
      "| 15000|    [21.0,1.0]|\n",
      "| 18000|    [23.0,2.0]|\n",
      "+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalized_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f131e24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fae4204",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data = finalized_data.randomSplit([0.75,0.25])\n",
    "regressor = LinearRegression(featuresCol=\"Input Features\",labelCol=\"salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c50c3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = regressor.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ad256a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11796.874999999998"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de66bd3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([83.8068, 1443.1818])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4705ca00",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = regressor.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52d60124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+------------------+\n",
      "|salary|Input Features|        prediction|\n",
      "+------+--------------+------------------+\n",
      "| 18000|    [23.0,2.0]|16610.795454545456|\n",
      "| 20000|    [24.0,3.0]|18137.784090909092|\n",
      "| 25000|    [30.0,8.0]| 25856.53409090908|\n",
      "| 30000|   [31.0,10.0]|28826.704545454537|\n",
      "+------+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee2e8d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1320.312499999999, 1877002.5584323339)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted.meanAbsoluteError,predicted.meanSquaredError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
